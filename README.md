![](LongBench/misc/logo.gif)
# ğŸ“š LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks
<p align="center">
    ğŸŒ <a href="https://longbench2.github.io" target="_blank">Project Page</a> â€¢ ğŸ“š <a href="https://arxiv.org/abs/2412.15204" target="_blank">LongBench v2 Paper</a> â€¢ ğŸ“Š <a href="https://huggingface.co/datasets/THUDM/LongBench-v2" target="_blank">LongBench v2 Dataset</a> â€¢ ğ• <a href="https://x.com/realYushiBai/status/1869946577349132766" target="_blank">Thread</a>
</p>
<p align="center">
    ğŸ“– <a href="https://arxiv.org/abs/2308.14508" target="_blank">LongBench Paper</a> â€¢ ğŸ¤— <a href="https://huggingface.co/datasets/THUDM/LongBench" target="_blank">LongBench Dataset</a>
</p>

**ğŸ“¢ The original LongBench v1 related files are moved under `LongBench/`, read its README [here](LongBench/README.md)**.

---

## ğŸ“Œ Proje AÃ§Ä±klamasÄ±
Bu betik, **LongBench** veri kÃ¼mesi Ã¼zerinde Ã§eÅŸitli LLM modelleri kullanarak tahminler Ã§alÄ±ÅŸtÄ±rmak iÃ§in geliÅŸtirilmiÅŸtir. OpenAI, Hugging Face ve Gemini modellerini destekler. AyrÄ±ca, bu repoyu fork ederek geliÅŸtirdim: [Forklanan Repo](https://github.com/iclal07/LongBenchv2)

---

## âš™ï¸ Kurulum AdÄ±mlarÄ±

### Gerekli BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kleyin
```sh
pip install -r requirements.txt
```

### API AnahtarlarÄ±nÄ± AyarlayÄ±n
`.env` dosyanÄ±zÄ± oluÅŸturun ve aÅŸaÄŸÄ±daki deÄŸiÅŸkenleri tanÄ±mlayÄ±n:
```env
API_KEY=your_openai_key
GEMINI_KEY=your_gemini_key
HF_TOKEN=your_huggingface_token
```

---

## ğŸš€ KullanÄ±m TalimatlarÄ±

### Temel KullanÄ±m
```sh
python pred.py --model gpt-4 --save_dir results
```

### Ã–zel API URL'si KullanÄ±mÄ±
```sh
python pred.py --model qwen --base_url http://custom-url.com/v1
```

ğŸ“Œ **Not:** EÄŸer `--base_url` girilmezse **varsayÄ±lan olarak** `http://localhost:8000/v1` kullanÄ±lacaktÄ±r.

---

## ğŸ”§ Parametre AÃ§Ä±klamalarÄ±

| Parametre | AÃ§Ä±klama | VarsayÄ±lan |
|-----------|----------|------------|
| `--base_url` | KullanÄ±lacak API adresi | `http://localhost:8000/v1` |
| `--model` | KullanÄ±lacak modelin ismi (`gpt-4`, `qwen`, `gemini-2.0-flash-exp` vb.) | `GLM-4-9B-Chat` |
| `--save_dir` | SonuÃ§larÄ±n kaydedileceÄŸi dizin | `results` |
| `--cot` | Chain of Thought (COT) aÃ§mak iÃ§in flag | `False` |
| `--no_context` | Modelin baÄŸlam kullanmadan Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlamak iÃ§in flag | `False` |
| `--rag` | KaÃ§ tane baÄŸlamsal veri kullanÄ±lacaÄŸÄ±nÄ± belirtir | `0` |

---

## ğŸ“ Ã‡Ä±ktÄ±lar

- Tahminler `.jsonl` formatÄ±nda `save_dir` klasÃ¶rÃ¼ne kaydedilir.
- Ã‡Ä±ktÄ± Ã¶rneÄŸi:
  ```json
  {
    "_id": "123",
    "question": "What is AI?",
    "response": "AI stands for Artificial Intelligence.",
    "pred": "B",
    "judge": true
  }
  ```

---

## ğŸ”¬ Deneysel Analizler
Bu Ã§alÄ±ÅŸmada **Gemini2.0 Flash Experimental** ve **Qwen2.5-14B-Instruct-1M** modellerini kullandÄ±m ve iki farklÄ± ayar altÄ±nda sonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±rdÄ±m:

### SonuÃ§lar:
1. **CoT olmadan yapÄ±lan deney:**
   ![CoT Olmadan](exp-wo-cot.png)
2. **CoT ile yapÄ±lan deney:**
   ![CoT ile](exp-cot.png)

---

## ğŸ› ï¸ Hata Giderme ve Loglama
- **Hata YÃ¶netimi:** API Ã§aÄŸrÄ±larÄ±nda hata olursa, 5 defaya kadar tekrar denenir.
- **Gecikme MekanizmasÄ±:** Gemini ve diÄŸer modeller iÃ§in **1-3 saniyelik rastgele bekleme sÃ¼resi** eklendi.
- **Hata mesajlarÄ±** konsola yazdÄ±rÄ±lÄ±r ve 1 saniye beklenerek tekrar denenir.

---

## ğŸ“œ Lisans
MIT LisansÄ±


ğŸ”— **Herhangi bir sorun yaÅŸarsanÄ±z, issue aÃ§abilirsiniz!** ğŸš€
